import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

class AnomalyDetector:
    """D√©tecteur d'anomalies r√©seau utilisant Isolation Forest"""
    
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.label_encoders = {}
        self.feature_columns = []
        self.is_trained = False
        
    def _prepare_features(self, df, fit_encoders=False):
        """
        Pr√©pare les features pour l'entra√Ænement ou la pr√©diction
        
        Args:
            df: DataFrame avec les donn√©es r√©seau
            fit_encoders: Si True, ajuste les encodeurs (mode entra√Ænement)
        
        Returns:
            numpy array avec les features pr√©par√©es
        """
        if df.empty:
            return np.array([])
        
        # Copie des donn√©es pour √©viter les modifications
        data = df.copy()
        
        # Features temporelles
        data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)
        data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)
        data['day_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)
        data['day_cos'] = np.cos(2 * np.pi * data['day_of_week'] / 7)
        
        # Features num√©riques directes
        numeric_features = [
            'port', 'data_volume_mb', 'hour_sin', 'hour_cos', 
            'day_sin', 'day_cos', 'avg_data_volume', 'std_data_volume',
            'max_data_volume', 'unique_ports'
        ]
        
        # Features cat√©gorielles √† encoder
        categorical_features = ['device_type', 'protocol']
        
        # Initialisation du DataFrame de features
        features_df = pd.DataFrame()
        
        # Ajout des features num√©riques
        for feature in numeric_features:
            if feature in data.columns:
                features_df[feature] = data[feature].fillna(0)
        
        # Encodage des features cat√©gorielles
        for feature in categorical_features:
            if feature in data.columns:
                if fit_encoders:
                    # Mode entra√Ænement: cr√©er et ajuster l'encodeur
                    self.label_encoders[feature] = LabelEncoder()
                    features_df[f'{feature}_encoded'] = self.label_encoders[feature].fit_transform(
                        data[feature].fillna('unknown')
                    )
                else:
                    # Mode pr√©diction: utiliser l'encodeur existant
                    if feature in self.label_encoders:
                        # G√©rer les nouvelles valeurs non vues pendant l'entra√Ænement
                        def safe_transform(value):
                            try:
                                return self.label_encoders[feature].transform([value])[0]
                            except ValueError:
                                # Valeur non vue pendant l'entra√Ænement
                                return -1
                        
                        features_df[f'{feature}_encoded'] = data[feature].fillna('unknown').apply(safe_transform)
                    else:
                        # Pas d'encodeur disponible
                        features_df[f'{feature}_encoded'] = 0
        
        # Features d√©riv√©es
        if 'data_volume_mb' in data.columns and 'avg_data_volume' in data.columns:
            # √âcart par rapport √† la moyenne de l'appareil
            features_df['volume_deviation'] = (
                data['data_volume_mb'] - data['avg_data_volume']
            ).fillna(0)
            
            # Ratio par rapport √† la moyenne
            features_df['volume_ratio'] = (
                data['data_volume_mb'] / (data['avg_data_volume'] + 1)
            ).fillna(1)
        
        # Features de port
        if 'port' in data.columns:
            # Indication si le port est dans les ports communs
            common_ports = [22, 23, 25, 53, 80, 110, 143, 443, 993, 995, 3389, 5432, 3306]
            features_df['is_common_port'] = data['port'].isin(common_ports).astype(int)
            
            # Port dans les plages privil√©gi√©es/syst√®me
            features_df['is_system_port'] = (data['port'] <= 1024).astype(int)
            features_df['is_ephemeral_port'] = (data['port'] >= 32768).astype(int)
        
        # Conversion en numpy array
        self.feature_columns = features_df.columns.tolist()
        return features_df.values
    
    def train_model(self, df, contamination=0.1):
        """
        Entra√Æne le mod√®le Isolation Forest
        
        Args:
            df: DataFrame avec les donn√©es d'entra√Ænement
            contamination: Proportion estim√©e d'anomalies dans les donn√©es
        """
        if df.empty:
            raise ValueError("Le DataFrame d'entra√Ænement est vide")
        
        print(f"Entra√Ænement du mod√®le avec {len(df)} √©chantillons...")
        
        # Pr√©paration des features
        X = self._prepare_features(df, fit_encoders=True)
        
        if X.size == 0:
            raise ValueError("Aucune feature n'a pu √™tre extraite des donn√©es")
        
        # Normalisation des donn√©es
        X_scaled = self.scaler.fit_transform(X)
        
        # Configuration et entra√Ænement du mod√®le Isolation Forest
        self.model = IsolationForest(
            contamination=contamination,
            random_state=42,
            n_estimators=100,
            max_samples='auto',
            bootstrap=False
        )
        
        # Entra√Ænement
        self.model.fit(X_scaled)
        self.is_trained = True
        
        print("‚úÖ Mod√®le entra√Æn√© avec succ√®s!")
        
        # √âvaluation sur les donn√©es d'entra√Ænement si les labels sont disponibles
        if 'is_anomaly' in df.columns:
            predictions = self.model.predict(X_scaled)
            predictions_binary = (predictions == -1).astype(int)
            true_labels = df['is_anomaly'].astype(int)
            
            print("\nüìä √âvaluation sur les donn√©es d'entra√Ænement:")
            print(f"Anomalies d√©tect√©es: {predictions_binary.sum()}")
            print(f"Vraies anomalies: {true_labels.sum()}")
            
            if true_labels.sum() > 0:
                print("\nRapport de classification:")
                print(classification_report(true_labels, predictions_binary, 
                                          target_names=['Normal', 'Anomalie']))
    
    def detect_anomalies(self, df):
        """
        D√©tecte les anomalies dans les nouvelles donn√©es
        
        Args:
            df: DataFrame avec les donn√©es √† analyser
        
        Returns:
            DataFrame avec les anomalies d√©tect√©es et leurs scores
        """
        if not self.is_trained:
            raise ValueError("Le mod√®le n'a pas √©t√© entra√Æn√©. Appelez train_model() d'abord.")
        
        if df.empty:
            return pd.DataFrame()
        
        print(f"D√©tection d'anomalies sur {len(df)} √©chantillons...")
        
        # Pr√©paration des features
        X = self._prepare_features(df, fit_encoders=False)
        
        if X.size == 0:
            return pd.DataFrame()
        
        # Normalisation
        X_scaled = self.scaler.transform(X)
        
        # Pr√©diction
        predictions = self.model.predict(X_scaled)
        anomaly_scores = self.model.score_samples(X_scaled)
        
        # Cr√©ation du DataFrame de r√©sultats
        results = df.copy()
        results['predicted_anomaly'] = (predictions == -1).astype(int)
        results['anomaly_score'] = anomaly_scores
        
        # Normalisation du score d'anomalie (plus n√©gatif = plus anormal)
        # Conversion vers un score entre 0 et 1 (1 = tr√®s anormal)
        min_score = anomaly_scores.min()
        max_score = anomaly_scores.max()
        if max_score != min_score:
            results['anomaly_confidence'] = 1 - (anomaly_scores - min_score) / (max_score - min_score)
        else:
            results['anomaly_confidence'] = 0.5
        
        # Filtrage pour ne garder que les anomalies d√©tect√©es
        anomalies = results[results['predicted_anomaly'] == 1].copy()
        
        if not anomalies.empty:
            # Tri par score d'anomalie (plus suspects en premier)
            anomalies = anomalies.sort_values('anomaly_confidence', ascending=False)
            
            # Classification du niveau de criticit√©
            def classify_severity(confidence):
                if confidence >= 0.8:
                    return 'Critique'
                elif confidence >= 0.6:
                    return '√âlev√©'
                elif confidence >= 0.4:
                    return 'Moyen'
                else:
                    return 'Faible'
            
            anomalies['severity'] = anomalies['anomaly_confidence'].apply(classify_severity)
            
            print(f"üö® {len(anomalies)} anomalies d√©tect√©es!")
            print(f"   - Critiques: {len(anomalies[anomalies['severity'] == 'Critique'])}")
            print(f"   - √âlev√©es: {len(anomalies[anomalies['severity'] == '√âlev√©'])}")
            print(f"   - Moyennes: {len(anomalies[anomalies['severity'] == 'Moyen'])}")
            print(f"   - Faibles: {len(anomalies[anomalies['severity'] == 'Faible'])}")
        else:
            print("‚úÖ Aucune anomalie d√©tect√©e.")
        
        return anomalies
    
    def get_model_info(self):
        """Retourne des informations sur le mod√®le entra√Æn√©"""
        if not self.is_trained:
            return {"status": "Non entra√Æn√©"}
        
        return {
            "status": "Entra√Æn√©",
            "features_count": len(self.feature_columns),
            "features": self.feature_columns,
            "model_type": "Isolation Forest",
            "n_estimators": self.model.n_estimators,
            "contamination": self.model.contamination
        }
